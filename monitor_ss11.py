import os
import requests
import urllib3
from datetime import datetime
from urllib.parse import urlparse, parse_qs

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# === é…ç½®åŒºï¼ˆæ‰‹åŠ¨æ›´æ–°è¿™é‡Œï¼ï¼‰===
# æ›¿æ¢ä¸ºå½“å‰æœ€æ–°å…¬å‘Šçš„ IDï¼ˆä»é“¾æ¥ä¸­æå–ï¼‰
LAST_KNOWN_CN_ID = "1"   # ğŸ‘ˆ å›½æœæœ€æ–°å…¬å‘Š ID
LAST_KNOWN_EN_ID = "i9ncluYb82HD"   # ğŸ‘ˆ å›½é™…æœæœ€æ–°å…¬å‘Š IDï¼ˆè‹¥æ— ï¼Œå¯è®¾ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰

CN_NEWSLIST_URL = "https://website.xdcdn.net/form/website/torchlight/news_cn.json"
EN_NEWSLIST_URL = "https://website.xdcdn.net/form/website/torchlight/news.json"

SENDKEY = os.getenv("SENDKEY")
GIST_TOKEN = os.getenv("GIST_TOKEN")

def send_wechat(title, link, prefix=""):
    if not SENDKEY:
        print("âŒ æœªè®¾ç½® SENDKEYï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")
        return False
    url = f"https://sctapi.ftqq.com/{SENDKEY}.send"
    data = {
        "title": f"{prefix}ã€ç«ç‚¬ä¹‹å…‰ SS11ã€‘{title}",
        "desp": f"[æŸ¥çœ‹å…¬å‘Š]({link})\n\n> æ£€æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    }
    try:
        resp = requests.post(url, data=data, timeout=10)
        success = resp.status_code == 200 and resp.json().get("code") == 0
        print(f"âœ… å¾®ä¿¡æ¨é€æˆåŠŸ: {title}" if success else f"âš ï¸ æ¨é€å¤±è´¥: {resp.text}")
        return success
    except Exception as e:
        print(f"âš ï¸ å¾®ä¿¡æ¨é€å¼‚å¸¸: {e}")
        return False


def save_to_gist(title, link, content, region):
    if not GIST_TOKEN:
        print("âš ï¸ æœªè®¾ç½® GIST_TOKENï¼Œè·³è¿‡ä¿å­˜å¿«ç…§")
        return False
    headers = {"Authorization": f"token {GIST_TOKEN}"}
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    filename = f"torchlight-{region}-{now.replace(':', '-')}.txt"
    data = {
        "description": f"ã€{region}ã€‘{title}",
        "public": False,
        "files": {
            filename: {
                "content": f"æ ‡é¢˜: {title}\né“¾æ¥: {link}\næ—¶é—´: {now}\n\n---\n\n{content}"
            }
        }
    }
    try:
        resp = requests.post("https://api.github.com/gists", headers=headers, json=data, timeout=15)
        if resp.status_code == 201:
            gist_id = resp.json()["id"]
            print(f"âœ… å…¨æ–‡å¿«ç…§å·²ä¿å­˜è‡³ Gist: https://gist.github.com/{gist_id}")
            return True
        else:
            print(f"âš ï¸ Gist ä¿å­˜å¤±è´¥: {resp.text}")
            return False
    except Exception as e:
        print(f"âš ï¸ Gist ä¿å­˜å¼‚å¸¸: {e}")
        return False


def extract_news_id(detail_url: str) -> str:
    parsed = urlparse(detail_url)
    return parse_qs(parsed.query).get("id", [None])[0]


def fetch_news_json(news_id: str, region: str = "cn") -> str:
    if not news_id:
        return "âš ï¸ æ— æ•ˆå…¬å‘Š ID"
    folder = "news_cn" if region == "cn" else "news/en"
    json_url = f"http://website.xdcdn.net/form/website/torchlight/{folder}/{news_id}.json"
    try:
        print(f"ğŸ“¥ è·å–å…¬å‘Š JSON: {json_url}")
        resp = requests.get(
            json_url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=10,
            verify=False,
            proxies={"http": None, "https": None}
        )
        resp.raise_for_status()
        data = resp.json()
        content_html = data.get("content", "")
        if not content_html:
            return "âš ï¸ JSON ä¸­æ—  content å­—æ®µ"

        from bs4 import BeautifulSoup
        soup = BeautifulSoup(content_html, "html.parser")
        text = soup.get_text(separator="\n", strip=True)
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        clean_text = "\n\n".join(lines)
        return clean_text[:8000] + ("\n\n...ï¼ˆå†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼‰" if len(clean_text) > 8000 else "")
    except Exception as e:
        return f"âš ï¸ è·å–å…¬å‘Šå¤±è´¥: {str(e)}"


def main():
    updated = False

    # === å›½æœæ£€æŸ¥ ===
    try:
        cn_resp = requests.get(CN_NEWSLIST_URL, timeout=10, verify=False)
        cn_data = cn_resp.json()
        announcements = cn_data.get("zh-cn", {}).get("announcement", [])
        if announcements:
            latest = announcements[0]
            cn_title = latest["title"]
            cn_link = latest["link"]
            cn_id = extract_news_id(cn_link)

            if cn_id and cn_id != LAST_KNOWN_CN_ID:
                print(f"ğŸ†• å‘ç°å›½æœæ–°å…¬å‘Š: {cn_title}")
                full_content = fetch_news_json(cn_id, "cn")
                # send_wechat(cn_title, cn_link, "ã€å›½æœã€‘")
                save_to_gist(cn_title, cn_link, full_content, "å›½æœ")
                updated = True
            else:
                print("ğŸ” å›½æœæ— æ–°å…¬å‘Š")
        else:
            print("âš ï¸ å›½æœå…¬å‘Šåˆ—è¡¨ä¸ºç©º")
    except Exception as e:
        print(f"âš ï¸ å›½æœæ£€æŸ¥å¤±è´¥: {e}")

    # === å›½é™…æœæ£€æŸ¥ ===
    try:
        en_resp = requests.get(EN_NEWSLIST_URL, timeout=10, verify=False)
        en_data = en_resp.json()
        announcements = en_data.get("en", {}).get("announcement", [])
        if announcements:
            latest = announcements[0]
            en_title = latest["title"]
            en_link = latest["link"]
            en_id = extract_news_id(en_link)

            if en_id and en_id != LAST_KNOWN_EN_ID:
                print(f"ğŸŒ å‘ç°å›½é™…æœæ–°å…¬å‘Š: {en_title}")
                full_content = fetch_news_json(en_id, "en")
                # send_wechat(en_title, en_link, "ã€å›½é™…æœã€‘")
                save_to_gist(en_title, en_link, full_content, "å›½é™…æœ")
                updated = True
            else:
                print("ğŸ” å›½é™…æœæ— æ–°å…¬å‘Š")
        else:
            print("âš ï¸ å›½é™…æœå…¬å‘Šåˆ—è¡¨ä¸ºç©º")
    except Exception as e:
        print(f"âš ï¸ å›½é™…æœæ£€æŸ¥å¤±è´¥: {e}")

    if not updated:
        print("âœ… æ— æ–°å…¬å‘Š")


if __name__ == "__main__":
    main()
